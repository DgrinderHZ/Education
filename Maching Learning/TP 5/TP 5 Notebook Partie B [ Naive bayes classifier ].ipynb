{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Classifieur Bayesien Naïf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn import datasets\n",
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "Créez une fonction CBN(X,Y) qui prend en entrée des données X et des étiquettes Y et\n",
    "qui renvoie une étiquette, pour chaque donnée, prédite à partir de la classe la plus\n",
    "probable selon l’équation (1). Ici encore, on prend chaque donnée, une par une, comme\n",
    "donnée de test et on considère toutes les données comme données d’apprentissage. Il est\n",
    "conseillé de calculer d’abord les barycentres et les probabilités à priori P (ωk ) pour\n",
    "chaque classe, puis de calculer les probabilités conditionnelles P (xi /ωk ) pour chaque\n",
    "classe et chaque variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CBN(x, y):\n",
    "    \"\"\"\n",
    "    Naive bayes classifier\n",
    "    \n",
    "     x : data\n",
    "     y : target (classe)\n",
    "    \"\"\"\n",
    "\n",
    "    # Constantes\n",
    "    nb_class = np.unique(y).size\n",
    "    data_size = len(x)\n",
    "    nb_features = x.shape[1]\n",
    "    # probabilité de chaque classe\n",
    "    proba_class = [e / float(len(x)) for e in Counter(y).values()] # trouver le ratio = probabilité\n",
    "\n",
    "    # cbn_target represente les nouvelles etiquettes\n",
    "    nbc_target = np.zeros(data_size, dtype=np.int)\n",
    "\n",
    "    # 1 folds Cross validation\n",
    "    # Choisi une observation parmis les donnees\n",
    "    # Genere la matrice d'apprentissge en excluant l'observation precedente\n",
    "    for i in range(data_size):\n",
    "        # leave one out\n",
    "        valid = x[i]\n",
    "        train = np.delete(x, i, 0)\n",
    "        train_target = np.delete(y, i, 0)\n",
    "\n",
    "        # Regroupe les donnees par classe et calcule leurs barycentres (means)\n",
    "        data_pclass = {} # dict -> classId: features of it\n",
    "        mean_pclass = {} # moyenne de chaque classe\n",
    "        for j in range(nb_class):\n",
    "            data_pclass[j] = [e for k, e in enumerate(train) if train_target[k] == j]\n",
    "            mean_pclass[j] = np.mean(a=data_pclass[j], axis=0)\n",
    "\n",
    "        # Calcule des distances\n",
    "        # dist represente la distance entre la donnee x et le barycentre pour chaque classe\n",
    "        # dist_total est la somme des distances par classe\n",
    "        dist = [abs(valid-barycentre) for barycentre in mean_pclass.values()]\n",
    "        dist_total = np.sum(dist, axis=0)\n",
    "\n",
    "        # Calcule de la probabilite PROP(P(xi/wk)P(wk))\n",
    "        # xi/wk = une donne x avec la valeur xi pour la variable i de la classe wk\n",
    "        '''P (xi /ωk ) : est la probabilité qu’une donnée x ait la valeur xi pour la variable i, si on connaît\n",
    "        sa classe ωk. Nous allons calculer cette probabilité en calculant la distance entre la donnée xi et\n",
    "        chaque barycentre des classes, divisée par la somme des distances entre cette donnée et chaque\n",
    "        barycentre.'''\n",
    "        tmp = (1-(dist/dist_total))\n",
    "        # pour chaque classe, multiplier  P(xi /ωk ) par P(wk)\n",
    "        for ii in range(len(tmp)): #classe\n",
    "            for j in range(len(tmp[ii])): # xi\n",
    "                tmp[ii][j] = tmp[ii][j]*proba_class[ii] # proba_class[ii] is 1/3 cuz we have 3 classes\n",
    "        # produit\n",
    "        pro = []\n",
    "        for ii in range(len(tmp)):\n",
    "            pro.append(tmp[ii].prod())\n",
    "        pro\n",
    "        \n",
    "        \n",
    "        nbc_target[i] = np.argmax(pro)\n",
    "\n",
    "    return {'target': nbc_target}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "        2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2,\n",
       "        2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jeu de donnee\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris ()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "nbc_target = CBN(x=X, y=Y)\n",
    "nbc_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.\n",
    "La fonction CBN calcule une étiquette prédite pour chaque donnée. Modifiez la fonction\n",
    "pour calculer et renvoyer l’erreur de prédiction : c’est à dire le pourcentage d’étiquettes\n",
    "mal prédites. Testez sur les données Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateur_malprediction(lis_1, lis_2):\n",
    "    \"\"\" validateur_malprediction : Retourne le pourcentage d’étiquettes mal prédites\n",
    "    \"\"\"\n",
    "    size = len(lis_1)\n",
    "    if size != len(lis_2):\n",
    "        logging.error(\"Liste de taille differente\")\n",
    "        return 0\n",
    "    # 1) trouver le total d’étiquettes mal prédites\n",
    "    _sum = 0\n",
    "    for i in range(size):\n",
    "        if lis_1[i] != lis_2[i]:\n",
    "            _sum += 1\n",
    "    # 2) retourner le facteur en %\n",
    "    return _sum*100 / float(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CBN_err(x, y):\n",
    "    \"\"\"\n",
    "    Naive bayes classifier\n",
    "    \n",
    "     x : data\n",
    "     y : target (classe)\n",
    "    \"\"\"\n",
    "\n",
    "    # Constantes\n",
    "    nb_class = np.unique(y).size\n",
    "    data_size = len(x)\n",
    "    nb_features = x.shape[1]\n",
    "    # probabilité de chaque classe\n",
    "    proba_class = [e / float(len(x)) for e in Counter(y).values()] # trouver le ratio = probabilité\n",
    "\n",
    "    # cbn_target represente les nouvelles etiquettes\n",
    "    nbc_target = np.zeros(data_size, dtype=np.int)\n",
    "\n",
    "    # 1 folds Cross validation\n",
    "    # Choisi une observation parmis les donnees\n",
    "    # Genere la matrice d'apprentissge en excluant l'observation precedente\n",
    "    for i in range(data_size):\n",
    "        # leave one out\n",
    "        valid = x[i]\n",
    "        train = np.delete(x, i, 0)\n",
    "        train_target = np.delete(y, i, 0)\n",
    "\n",
    "        # Regroupe les donnees par classe et calcule leurs barycentres (means)\n",
    "        data_pclass = {} # dict -> classId: features of it\n",
    "        mean_pclass = {} # moyenne de chaque classe\n",
    "        for j in range(nb_class):\n",
    "            data_pclass[j] = [e for k, e in enumerate(train) if train_target[k] == j]\n",
    "            mean_pclass[j] = np.mean(a=data_pclass[j], axis=0)\n",
    "\n",
    "        # Calcule des distances\n",
    "        # dist represente la distance entre la donnee x et le barycentre pour chaque classe\n",
    "        # dist_total est la somme des distances par classe\n",
    "        dist = [abs(valid-barycentre) for barycentre in mean_pclass.values()]\n",
    "\n",
    "        dist_total = np.sum(dist, axis=0)\n",
    "        # Calcule de la probabilite PROP(P(xi/wk)P(wk))\n",
    "        # xi/wk = une donne x avec la valeur xi pour la variable i de la classe wk\n",
    "        '''P (xi /ωk ) : est la probabilité qu’une donnée x ait la valeur xi pour la variable i, si on connaît\n",
    "        sa classe ωk. Nous allons calculer cette probabilité en calculant la distance entre la donnée xi et\n",
    "        chaque barycentre des classes, divisée par la somme des distances entre cette donnée et chaque\n",
    "        barycentre.'''\n",
    "        tmp = (1-(dist/dist_total))\n",
    "        # pour chaque classe, multiplier  P(xi /ωk ) par P(wk)\n",
    "        for ii in range(len(tmp)): #classe\n",
    "            for j in range(len(tmp[ii])): # xi\n",
    "                tmp[ii][j] = tmp[ii][j]*proba_class[ii] # proba_class[ii] is 1/3 cuz we have 3 classes\n",
    "\n",
    "        # produit\n",
    "        pro = []\n",
    "        for ii in range(len(tmp)):\n",
    "            pro.append(tmp[ii].prod())\n",
    "        \n",
    "        nbc_target[i] = np.argmax(pro)\n",
    "\n",
    "    return {'target': nbc_target, 'error': validateur_malprediction(nbc_target, y)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1\n",
      " 1 2 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 2 2 2\n",
      " 2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 1 2 2 2 2 2\n",
      " 2 2]\n",
      "Err: 12.67%\n"
     ]
    }
   ],
   "source": [
    "nbc = CBN_err(x=X, y=Y)\n",
    "print(\"Target\", nbc[\"target\"])\n",
    "print(\"Err: %.2f%%\"% nbc[\"error\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.\n",
    "Testez la fonction du Classifieur Bayesien Naïf inclut dans sklearn. Cette fonction\n",
    "utilise une distribution Gaussienne au lieu des distances aux barycentres. Les résultats\n",
    "sont-ils différents ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predic_cross_valid(algo, x, y):\n",
    "    \"\"\"train : cross validation pour l'algorithme de sklearn\n",
    "        x = data\n",
    "        y = target\n",
    "    \"\"\"\n",
    "    # taille de jeu de donnees\n",
    "    taille = len(x)\n",
    "\n",
    "    predicted_target = np.zeros(taille, dtype=np.int)\n",
    "\n",
    "    for i in range(taille):\n",
    "        # leave one out\n",
    "        valid = x[i]\n",
    "        train = np.delete(x, i, 0)\n",
    "        train_target = np.delete(y, i, 0)\n",
    "        \n",
    "        # trouver la classe\n",
    "        algo.fit(train, train_target)\n",
    "        predicted_target[i] = algo.predict(valid.reshape(1, -1))\n",
    "\n",
    "    return {'target': predicted_target, 'error': validateur_malprediction(predicted_target, y)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBN : %s [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1\n",
      " 1 2 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 2 2 2\n",
      " 2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 1 2 2 2 2 2\n",
      " 2 2]\n",
      "CBN pourcentage d’étiquettes mal prédites : 12.7%\n",
      "____________________________________________________________________________________________________\n",
      "Naive Bayes GaussianNB : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "GaussianNB pourcentage d’étiquettes mal prédites : 4.7%\n"
     ]
    }
   ],
   "source": [
    "clf = naive_bayes.GaussianNB()\n",
    "sklearn_nbc_target = predic_cross_valid(clf, X, Y)\n",
    "\n",
    "print(\"CBN : %s\", nbc['target'])\n",
    "print(\"CBN pourcentage d’étiquettes mal prédites : %.1f%%\"% nbc['error'])\n",
    "def sep(n):\n",
    "    print(\"_\"*n)\n",
    "sep(100)\n",
    "print(\"Naive Bayes GaussianNB : %s\"% sklearn_nbc_target['target'])\n",
    "print(\"GaussianNB pourcentage d’étiquettes mal prédites : %.1f%%\"% sklearn_nbc_target['error'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pourcentages différents!\n",
    "\n",
    "On va les rendrent égaux par suite!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. \n",
    "\n",
    "Modifiez la fonction CBN pour qu’elle utilise une distribution Gaussienne pour les lois\n",
    "de probabilités au lieu de simple distance au barycentre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp, sqrt, pi\n",
    "# Calculate the Gaussian probability distribution function for x\n",
    "def calculate_probability(x, mean, stdev):\n",
    "    exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "    return (1 / (sqrt(2 * pi) * stdev)) * exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CBN_GAUSS(x, y):\n",
    "    \"\"\"\n",
    "    Naive bayes classifier\n",
    "    \n",
    "     x : data\n",
    "     y : target (classe)\n",
    "    \"\"\"\n",
    "\n",
    "    # Constantes\n",
    "    nb_class = np.unique(y).size\n",
    "    data_size = len(x)\n",
    "    nb_features = x.shape[1]\n",
    "    # probabilité de chaque classe\n",
    "    proba_class = [e / float(len(x)) for e in Counter(y).values()] # trouver le ratio = probabilité\n",
    "\n",
    "    # cbn_target represente les nouvelles etiquettes\n",
    "    nbc_target = np.zeros(data_size, dtype=np.int)\n",
    "\n",
    "    # 1 folds Cross validation\n",
    "    # Choisi une observation parmis les donnees\n",
    "    # Genere la matrice d'apprentissge en excluant l'observation precedente\n",
    "    for i in range(data_size):\n",
    "        # leave one out\n",
    "        valid = x[i]\n",
    "        train = np.delete(x, i, 0)\n",
    "        train_target = np.delete(y, i, 0)\n",
    "\n",
    "        # Regroupe les donnees par classe et calcule leurs moyenne et ecart-type\n",
    "        data_pclass = {} # dict -> classId: features of it\n",
    "        mean_pclass = {} # moyenne de chaque classe\n",
    "        std_pclass = {} # moyenne de chaque classe\n",
    "        for j in range(nb_class):\n",
    "            data_pclass[j] = [e for k, e in enumerate(train) if train_target[k] == j]\n",
    "            mean_pclass[j] = np.mean(a=data_pclass[j], axis=0)\n",
    "            std_pclass[j] = np.std(a=data_pclass[j], axis=0)\n",
    "            \n",
    "        \n",
    "        # Calcule de la probabilite PROP(P(xi/wk), valid = xi\n",
    "        prob = []\n",
    "        for ii in range(nb_class): #classe\n",
    "            tmp = []\n",
    "            for indx in range(len(valid)):\n",
    "                tmp.append(calculate_probability(valid[indx], mean_pclass[ii][indx], std_pclass[ii][indx]))\n",
    "            prob.append(tmp)\n",
    "       \n",
    "        # pour chaque classe, multiplier  P(xi /ωk ) par P(wk)\n",
    "        for ii in range(nb_class): #classe\n",
    "            for j in range(len(prob[ii])): # xi\n",
    "                prob[ii][j] = prob[ii][j]*proba_class[ii] # proba_class[ii] is 1/3 cuz we have 3 classes\n",
    "        # produit\n",
    "    \n",
    "        product = []\n",
    "        for ii in range(len(prob)):\n",
    "            product.append(np.prod(prob[ii]))\n",
    "        \n",
    "        nbc_target[i] = np.argmax(product)\n",
    "\n",
    "    return {'target': nbc_target, 'error': validateur_malprediction(nbc_target, y)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++ [ distance au barycentre]\n",
      "CBN : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1\n",
      " 1 2 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 2 2 2\n",
      " 2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 1 2 2 2 2 2\n",
      " 2 2]\n",
      "CBN pourcentage d’étiquettes mal prédites : 12.7%\n",
      "____________________________________________________________________________________________________\n",
      "++++ [distribution Gaussienne]\n",
      "CBN : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "CBN pourcentage d’étiquettes mal prédites : 4.7%\n",
      "____________________________________________________________________________________________________\n",
      "++++ [distribution Gaussienne GaussianNB de scikit-learn]\n",
      "Naive Bayes : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "GaussianNB pourcentage d’étiquettes mal prédites : 4.7%\n"
     ]
    }
   ],
   "source": [
    "nbc_barycentre = nbc\n",
    "nbc_gauss = CBN_GAUSS(X, Y)\n",
    "clf = naive_bayes.GaussianNB()\n",
    "sklearn_nbc_target = predic_cross_valid(clf, X, Y)\n",
    "\n",
    "print(\"++++ [ distance au barycentre]\")\n",
    "print(\"CBN : %s\"% nbc_barycentre['target'])\n",
    "print(\"CBN pourcentage d’étiquettes mal prédites : %.1f%%\"% nbc_barycentre['error'])\n",
    "sep(100)\n",
    "print(\"++++ [distribution Gaussienne]\")\n",
    "print(\"CBN : %s\"% nbc_gauss['target'])\n",
    "print(\"CBN pourcentage d’étiquettes mal prédites : %.1f%%\"% nbc_gauss['error'])\n",
    "sep(100)\n",
    "print(\"++++ [distribution Gaussienne GaussianNB de scikit-learn]\")\n",
    "print(\"Naive Bayes : %s\"% sklearn_nbc_target['target'])\n",
    "print(\"GaussianNB pourcentage d’étiquettes mal prédites : %.1f%%\"% sklearn_nbc_target['error'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
